{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os\n",
    "import jsonlines\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133147</th>\n",
       "      <td>video_games</td>\n",
       "      <td>My friend got his about week 5. Got mine the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363746</th>\n",
       "      <td>video_games</td>\n",
       "      <td>This post has been removed.\\n\\n&amp;gt;All individ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71307</th>\n",
       "      <td>video_games</td>\n",
       "      <td>I'm average not in Legends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261637</th>\n",
       "      <td>sports</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399053</th>\n",
       "      <td>video_games</td>\n",
       "      <td>Don't give up hope, that would be awesome!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                cat                                                txt\n",
       "133147  video_games  My friend got his about week 5. Got mine the w...\n",
       "363746  video_games  This post has been removed.\\n\\n&gt;All individ...\n",
       "71307   video_games                         I'm average not in Legends\n",
       "261637       sports                                          [deleted]\n",
       "399053  video_games       Don't give up hope, that would be awesome!!!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('categorized-comments.jsonl') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "df_inter = pd.DataFrame(lines)\n",
    "df_inter.columns = ['json_element']\n",
    "\n",
    "df_inter['json_element'].apply(json.loads)\n",
    "\n",
    "df = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "#create random sample with n = to 5000\n",
    "df = df.sample(n=5000, random_state=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert column ot string\n",
    "df['txt'] = df['txt'].astype(str)\n",
    "\n",
    "#A. Convert all text to lowercase letters.\n",
    "df['txt']= df['txt'].str.lower()\n",
    "\n",
    "#B. Remove all punctuation from the text with regex\n",
    "df['txt'] = df['txt'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "#C. Remove stop words.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['txt'] = df['txt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D. Apply NLTKâ€™s PorterStemmer.\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# set stemmer function to variable to variable \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# function to stems words of dataframe\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# run function on datfrtame column -- takes awhile\n",
    "df['txt']  = df['txt'].apply(stem_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "feature_matrix = tfidf.fit_transform(df['txt'])\n",
    "\n",
    "#shar matrix as dense\n",
    "texts = feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['cat']\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y = LabelBinarizer().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7   3  19]\n",
      " [  5 129  94]\n",
      " [ 19  83 641]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.17      0.23        29\n",
      "           1       0.60      0.57      0.58       228\n",
      "           2       0.85      0.89      0.87       743\n",
      "\n",
      "   micro avg       0.79      0.80      0.79      1000\n",
      "   macro avg       0.59      0.54      0.56      1000\n",
      "weighted avg       0.78      0.80      0.78      1000\n",
      " samples avg       0.78      0.80      0.79      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmeiners\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1)))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Classifier with Keras\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "X = df['txt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          874000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 30003     \n",
      "=================================================================\n",
      "Total params: 904,003\n",
      "Trainable params: 30,003\n",
      "Non-trainable params: 874,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "25/25 [==============================] - 2s 61ms/step - loss: 0.8591 - acc: 0.7013 - f1_m: 0.7046 - precision_m: 0.6755 - recall_m: 0.7412 - val_loss: 0.7770 - val_acc: 0.7237 - val_f1_m: 0.7177 - val_precision_m: 0.7080 - val_recall_m: 0.7277\n",
      "Epoch 2/6\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 0.7308 - acc: 0.7247 - f1_m: 0.7317 - precision_m: 0.7193 - recall_m: 0.7447 - val_loss: 0.7374 - val_acc: 0.7237 - val_f1_m: 0.7161 - val_precision_m: 0.6957 - val_recall_m: 0.7377\n",
      "Epoch 3/6\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 0.6888 - acc: 0.7266 - f1_m: 0.7472 - precision_m: 0.7256 - recall_m: 0.7703 - val_loss: 0.7276 - val_acc: 0.7237 - val_f1_m: 0.7189 - val_precision_m: 0.6923 - val_recall_m: 0.7478\n",
      "Epoch 4/6\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 0.6603 - acc: 0.7281 - f1_m: 0.7605 - precision_m: 0.7295 - recall_m: 0.7944 - val_loss: 0.7284 - val_acc: 0.7250 - val_f1_m: 0.7186 - val_precision_m: 0.6966 - val_recall_m: 0.7422\n",
      "Epoch 5/6\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.6388 - acc: 0.7294 - f1_m: 0.7641 - precision_m: 0.7316 - recall_m: 0.8000 - val_loss: 0.7218 - val_acc: 0.7275 - val_f1_m: 0.7230 - val_precision_m: 0.6941 - val_recall_m: 0.7545\n",
      "Epoch 6/6\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 0.6211 - acc: 0.7300 - f1_m: 0.7687 - precision_m: 0.7331 - recall_m: 0.8081 - val_loss: 0.7227 - val_acc: 0.7275 - val_f1_m: 0.7186 - val_precision_m: 0.6917 - val_recall_m: 0.7478\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7365497350692749 0.7300000190734863 0.7370343804359436 0.7153397798538208 0.7607421875\n"
     ]
    }
   ],
   "source": [
    "print(loss, accuracy, f1_score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifying Images\n",
    "\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#Set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set image information\n",
    "channels = 1\n",
    "height = 28 \n",
    "width = 28\n",
    "\n",
    "# Load data and target from MNIST \n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], height, width, channels)\n",
    "\n",
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], height, width, channels)\n",
    "\n",
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "# Start neural network\n",
    "network = Sequential()\n",
    "\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters=64,                   \n",
    "            kernel_size=(5, 5),                   \n",
    "            input_shape=(width, height, channels),                   \n",
    "            activation='relu', padding='same'))\n",
    "\n",
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "# Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy                \n",
    "            optimizer=\"rmsprop\", # Root Mean Square Propagation                \n",
    "            metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features            \n",
    "            target_train, # Target            \n",
    "            epochs=2, # Number of epochs            \n",
    "            verbose=0, # Don't print description after each epoch            \n",
    "            batch_size=1000, # Number of observations per batch            \n",
    "            validation_data=(features_test, target_test)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.053312063217163, 1.9260090589523315],\n",
       " 'accuracy': [0.2892666757106781, 0.3621666729450226],\n",
       " 'val_loss': [1.9073302745819092, 1.8060158491134644],\n",
       " 'val_accuracy': [0.583899974822998, 0.6521999835968018]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report on accuracy of model\n",
    "history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
